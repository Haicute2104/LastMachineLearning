"""
Script Ä‘Ã¡nh giÃ¡ vÃ  so sÃ¡nh tá»•ng thá»ƒ cÃ¡c mÃ´ hÃ¬nh Machine Learning
"""
import joblib
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import os

print("=" * 70)
print("ÄÃNH GIÃ Tá»”NG THá»‚ CÃC MÃ” HÃŒNH MACHINE LEARNING")
print("=" * 70)

# Load dá»¯ liá»‡u
print("\n1. LOADING DATA...")
print("-" * 70)
data = pd.read_csv('bangkok-2016.csv')
data.columns = data.columns.str.strip()
data_cleaned = data.drop(columns=['ICT', 'Events', 'Max Gust SpeedKm/h'])
data_cleaned = data_cleaned.fillna(data_cleaned.mean())

X = data_cleaned.drop(columns=['Mean TemperatureC'])
y = data_cleaned['Mean TemperatureC']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)

print(f"âœ“ Dataset shape: {data_cleaned.shape}")
print(f"âœ“ Training set: {X_train.shape[0]} samples")
print(f"âœ“ Test set: {X_test.shape[0]} samples")
print(f"âœ“ Features: {X_train.shape[1]}")
print(f"âœ“ Target variable: Mean Temperature (Â°C)")
print(f"âœ“ Target range: {y_test.min():.1f}Â°C - {y_test.max():.1f}Â°C")

# Load models
print("\n2. LOADING TRAINED MODELS...")
print("-" * 70)

models = {}
metrics = {}

try:
    # Load Linear Regression
    models['Linear Regression'] = joblib.load('Linear.pkl')
    print("âœ“ Linear Regression model loaded")
    
    # Load Lasso Regression
    models['Lasso Regression'] = joblib.load('lasso.pkl')
    print("âœ“ Lasso Regression model loaded")
    
    # Load MLP Regressor
    models['MLP Regressor'] = joblib.load('MLP.pkl')
    scaler = joblib.load('scaler.pkl')
    print("âœ“ MLP Regressor model loaded")
    print("âœ“ StandardScaler loaded")
    
    # Load Stacking Regressor
    models['Stacking Regressor'] = joblib.load('Stacking.pkl')
    print("âœ“ Stacking Regressor model loaded")
    
except FileNotFoundError as e:
    print(f"âœ— Error: {e}")
    print("Please run hello.py first to train the models!")
    exit(1)

# Evaluate models
print("\n3. EVALUATING MODELS...")
print("-" * 70)

for model_name, model in models.items():
    print(f"\nğŸ“Š Evaluating {model_name}...")
    
    # Prepare data based on model type
    if model_name in ['MLP Regressor', 'Stacking Regressor']:
        X_test_scaled = scaler.transform(X_test)
        y_pred = model.predict(X_test_scaled)
    else:
        y_pred = model.predict(X_test)
    
    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    # Calculate additional metrics
    mean_abs_percent_error = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    max_error = np.max(np.abs(y_test - y_pred))
    min_error = np.min(np.abs(y_test - y_pred))
    std_error = np.std(y_test - y_pred)
    
    metrics[model_name] = {
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'RÂ²': r2,
        'MAPE': mean_abs_percent_error,
        'Max Error': max_error,
        'Min Error': min_error,
        'Std Error': std_error
    }
    
    print(f"  âœ“ MSE:  {mse:.4f}")
    print(f"  âœ“ RMSE: {rmse:.4f}Â°C")
    print(f"  âœ“ MAE:  {mae:.4f}Â°C")
    print(f"  âœ“ RÂ²:   {r2:.4f}")
    print(f"  âœ“ MAPE: {mean_abs_percent_error:.2f}%")

# Summary table
print("\n" + "=" * 70)
print("ğŸ“ˆ SUMMARY TABLE - MODEL COMPARISON")
print("=" * 70)

# Create summary DataFrame
summary_df = pd.DataFrame(metrics).T

# Round values
summary_df = summary_df.round(4)

print("\n" + summary_df.to_string())

# Find best models
print("\n" + "=" * 70)
print("ğŸ† BEST MODELS BY METRIC")
print("=" * 70)

best_rmse = summary_df['RMSE'].idxmin()
best_mae = summary_df['MAE'].idxmin()
best_r2 = summary_df['RÂ²'].idxmax()
best_mape = summary_df['MAPE'].idxmin()

print(f"\nâœ“ Best RMSE: {best_rmse} ({summary_df.loc[best_rmse, 'RMSE']:.4f}Â°C)")
print(f"âœ“ Best MAE:  {best_mae} ({summary_df.loc[best_mae, 'MAE']:.4f}Â°C)")
print(f"âœ“ Best RÂ²:   {best_r2} ({summary_df.loc[best_r2, 'RÂ²']:.4f})")
print(f"âœ“ Best MAPE: {best_mape} ({summary_df.loc[best_mape, 'MAPE']:.2f}%)")

# Model recommendations
print("\n" + "=" * 70)
print("ğŸ’¡ RECOMMENDATIONS")
print("=" * 70)

# Analyze which model is overall best
# Give more weight to RMSE and RÂ²
weighted_scores = {}
for model_name in summary_df.index:
    # Normalize metrics (lower is better for RMSE, MAE, MAPE; higher is better for RÂ²)
    rmse_score = 1 / (summary_df.loc[model_name, 'RMSE'] / summary_df['RMSE'].min())
    mae_score = 1 / (summary_df.loc[model_name, 'MAE'] / summary_df['MAE'].min())
    r2_score = summary_df.loc[model_name, 'RÂ²'] / summary_df['RÂ²'].max()
    mape_score = 1 / (summary_df.loc[model_name, 'MAPE'] / summary_df['MAPE'].min())
    
    # Weighted average (higher weight on RMSE and RÂ²)
    weighted_score = (rmse_score * 0.4 + mae_score * 0.2 + r2_score * 0.3 + mape_score * 0.1)
    weighted_scores[model_name] = weighted_score

best_overall = max(weighted_scores, key=weighted_scores.get)

print(f"\nğŸ¯ Overall Best Model: {best_overall}")
print(f"\nğŸ“ Reasons:")
print(f"  â€¢ {best_overall} has the best balance of accuracy and robustness")
print(f"  â€¢ RMSE: {summary_df.loc[best_overall, 'RMSE']:.4f}Â°C")
print(f"  â€¢ RÂ² Score: {summary_df.loc[best_overall, 'RÂ²']:.4f}")

print(f"\nğŸ’¼ Model Characteristics:")
for model_name in summary_df.index:
    print(f"\n  {model_name}:")
    print(f"    â€¢ Complexity: {'High' if 'Stacking' in model_name or 'MLP' in model_name else 'Low'}")
    print(f"    â€¢ Training Time: {'Long' if 'MLP' in model_name or 'Stacking' in model_name else 'Fast'}")
    print(f"    â€¢ Prediction Time: {'Medium' if 'Stacking' in model_name or 'MLP' in model_name else 'Fast'}")
    print(f"    â€¢ Interpretability: {'Low' if 'MLP' in model_name or 'Stacking' in model_name else 'High'}")

# Model comparison chart
print("\n" + "=" * 70)
print("ğŸ“Š PERFORMANCE RANKING")
print("=" * 70)

# Sort by weighted score
sorted_models = sorted(weighted_scores.items(), key=lambda x: x[1], reverse=True)
for i, (model_name, score) in enumerate(sorted_models, 1):
    print(f"{i}. {model_name} (Score: {score:.4f})")

print("\n" + "=" * 70)
print("âœ… EVALUATION COMPLETE!")
print("=" * 70)

